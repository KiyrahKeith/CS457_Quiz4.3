{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "# Introduction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "In this lecture, we will build a neural network from scratch and code how it performs predictions using forward propagation. Please note that all deep learning libraries have the entire training and prediction processes implemented, and so in practice you wouldn't really need to build a neural network from scratch. However, hopefully completing this lab will help you understand neural networks and how they work even better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "## Recap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "From the lectures, let's recap how a neural network makes predictions through the forward propagation process. Here is a neural network that takes two inputs, has one hidden layer with two nodes, and an output layer with one node."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<img src=\"http://cocl.us/neural_network_example\" alt=\"Neural Network Example\" width=600px>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Let's start by randomly initializing the weights and the biases in the network. We have 6 weights and 3 biases, one for each node in the hidden layer as well as for each node in the output layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np # import Numpy library to generate \n",
    "\n",
    "weights = np.around(np.random.uniform(size=6), decimals=2) # initialize the weights\n",
    "biases = np.around(np.random.uniform(size=3), decimals=2) # initialize the biases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Let's print the weights and biases for sanity check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "print(weights)\n",
    "print(biases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Input layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Now that we have the weights and the biases defined for the network, let's compute the output for a given input, $x_1$ and $x_2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "x_1 = 0.5 # input 1\n",
    "x_2 = 0.85 # input 2\n",
    "\n",
    "print('x1 is {} and x2 is {}'.format(x_1, x_2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Let's start by computing the wighted sum of the inputs, $z_{1, 1}$, at the first node of the hidden layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "z_11 = x_1 * weights[0] + x_2 * weights[1] + biases[0]\n",
    "\n",
    "print('<Your name> knew that the weighted sum of the inputs at the first node in the hidden layer is {}'.format(z_11))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Next, let's compute the weighted sum of the inputs, $z_{1, 2}$, at the second node of the hidden layer. Assign the value to **z_12**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "### type code here\n",
    "z_12 = x_1 * weights[2] + x_2 * weights[3] + biases[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Double-click __here__ for the solution.\n",
    "<!-- The correct answer is:\n",
    "z_12 = x_1 * weights[2] + x_2 * weights[3] + biases[1]\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Print the weighted sum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "print('<Your name> knew that the weighted sum of the inputs at the second node in the hidden layer is {}'.format(np.around(z_12, decimals=4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hidden layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Next, assuming a **sigmoid** activation function, let's compute the activation of the first node, $a_{1, 1}$, in the hidden layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "a_11 = 1.0 / (1.0 + np.exp(-z_11))\n",
    "\n",
    "print('The activation of the first node in the hidden layer is {}'.format(np.around(a_11, decimals=4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Let's also compute the activation of the second node, $a_{1, 2}$, in the hidden layer. Assign the value to **a_12**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "### type code here\n",
    "\n",
    "a_12 = 1.0 / (1.0 + np.exp(-z_12))\n",
    "\n",
    "print('The activation of the first node in the hidden layer is {}'.format(np.around(a_12, decimals=4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Print the activation of the second node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "print('The activation of the second node in the hidden layer is {}'.format(np.around(a_12, decimals=4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Output layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Now these activations will serve as the inputs to the output layer. So, let's compute the weighted sum of these inputs to the node in the output layer. Assign the value to **z_2**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "### type code here\n",
    "z_2 = a_11 * weights[4] + a_12 * weights[5] + biases[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Print the weighted sum of the inputs at the node in the output layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "print('The weighted sum of the inputs at the node in the output layer is {}'.format(np.around(z_2, decimals=4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Finally, let's compute the output of the network as the activation of the node in the output layer. Assign the value to **a_2**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "### type code here\n",
    "a_2 = 1.0/(1.0 + np.exp(-z_2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Print the activation of the node in the output layer which is equivalent to the prediction made by the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "print('The output of the network for x1 = 0.5 and x2 = 0.85 is {}'.format(np.around(a_2, decimals=4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Obviously, neural networks for real problems are composed of many hidden layers and many more nodes in each layer. So, we can't continue making predictions using this very inefficient approach of computing the weighted sum at each node and the activation of each node manually. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "In order to code an automatic way of making predictions, let's generalize our network. A general network would take $n$ inputs, would have many hidden layers, each hidden layer having $m$ nodes, and would have an output layer. Although the network is showing one hidden layer, but we will code the network to have many hidden layers. Similarly, although the network shows an output layer with one node, we will code the network to have more than one node in the output layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://cocl.us/general_neural_network\" alt=\"Neural Network General\" width=600px>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='item12'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "## Practice: Initialize a Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Let's start by formally defining the structure of the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "n = 2 # number of inputs\n",
    "num_hidden_layers = 2 # number of hidden layers\n",
    "m = [2, 2] # number of nodes in each hidden layer\n",
    "num_nodes_output = 2 # number of nodes in the output layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Now that we defined the structure of the network, let's go ahead and inititailize the weights and the biases in the network to random numbers. In order to be able to initialize the weights and the biases to random numbers, we will need to import the **Numpy** library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np # import the Numpy library\n",
    "\n",
    "num_nodes_previous = n # number of nodes in the previous layer\n",
    "\n",
    "network = {} # initialize network an an empty dictionary\n",
    "\n",
    "# loop through each layer and randomly initialize the weights and biases associated with each node\n",
    "# notice how we are adding 1 to the number of hidden layers in order to include the output layer\n",
    "for layer in range(num_hidden_layers + 1): \n",
    "    \n",
    "    # determine name of layer\n",
    "    if layer == num_hidden_layers:\n",
    "        layer_name = 'output'\n",
    "        num_nodes = num_nodes_output\n",
    "    else:\n",
    "        layer_name = 'layer_{}'.format(layer + 1)\n",
    "        num_nodes = m[layer]\n",
    "    \n",
    "    # initialize weights and biases associated with each node in the current layer\n",
    "    network[layer_name] = {}\n",
    "    for node in range(num_nodes):\n",
    "        node_name = 'node_{}'.format(node+1)\n",
    "        network[layer_name][node_name] = {\n",
    "            'weights': np.around(np.random.uniform(size=num_nodes_previous), decimals=2),\n",
    "            'bias': np.around(np.random.uniform(size=1), decimals=2),\n",
    "        }\n",
    "    \n",
    "    num_nodes_previous = num_nodes\n",
    "    \n",
    "print(network) # print network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Awesome! So now with the above code, we are able to initialize the weights and the biases pertaining to any network of any number of hidden layers and number of nodes in each layer. But let's put this code in a function so that we are able to repetitively execute all this code whenever we want to construct a neural network.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "def initialize_network(num_inputs, num_hidden_layers, num_nodes_hidden, num_nodes_output):\n",
    "    \n",
    "    num_nodes_previous = num_inputs # number of nodes in the previous layer\n",
    "\n",
    "    network = {}\n",
    "    \n",
    "    # loop through each layer and randomly initialize the weights and biases associated with each layer\n",
    "    for layer in range(num_hidden_layers + 1):\n",
    "        \n",
    "        if layer == num_hidden_layers:\n",
    "            layer_name = 'output' # name last layer in the network output\n",
    "            num_nodes = num_nodes_output\n",
    "        else:\n",
    "            layer_name = 'layer_{}'.format(layer + 1) # otherwise give the layer a number\n",
    "            num_nodes = num_nodes_hidden[layer] \n",
    "        \n",
    "        # initialize weights and bias for each node\n",
    "        network[layer_name] = {}\n",
    "        for node in range(num_nodes):\n",
    "            node_name = 'node_{}'.format(node+1)\n",
    "            network[layer_name][node_name] = {\n",
    "                'weights': np.around(np.random.uniform(size=num_nodes_previous), decimals=2),\n",
    "                'bias': np.around(np.random.uniform(size=1), decimals=2),\n",
    "            }\n",
    "    \n",
    "        num_nodes_previous = num_nodes\n",
    "\n",
    "    return network # return the network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "#### Use the *initialize_network* function to create a network that:\n",
    "\n",
    "1. takes 5 inputs\n",
    "2. has three hidden layers\n",
    "3. has 3 nodes in the first layer, 2 nodes in the second layer, and 3 nodes in the third layer\n",
    "4. has 1 node in the output layer\n",
    "\n",
    "Call the small network **Network**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "### type code here\n",
    "network = initialize_network(5, 3, [3,2,3], 1)\n",
    "network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<a id=\"item3\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='item13'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "## Compute Weighted Sum at Each Node"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "The weighted sum at each node is computed as the dot product of the inputs and the weights plus the bias. So let's create a function called *compute_weighted_sum* that does just that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "def compute_weighted_sum(inputs, weights, bias):\n",
    "#     return np.sum(np.dot(inputs,weights)) + bias\n",
    "    return np.sum(inputs * weights) + bias\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Let's generate 5 inputs that we can feed to **small_network**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "from random import seed\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(12)\n",
    "inputs = np.around(np.random.uniform(size=5), decimals=2)\n",
    "\n",
    "print('The inputs to the network are {}'.format(inputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "#### Use the *compute_weighted_sum* function to compute the weighted sum at the first node in the first hidden layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "### type code here\n",
    "weighted_sum = compute_weighted_sum(inputs ,network['layer_1']['node_1']['weights'],network['layer_1']['node_1']['bias'])\n",
    "print('The weighted sum at the first node in the hidden layer is {}'.format(np.around(weighted_sum[0], decimals=4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<a id=\"item4\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='item14'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "## Compute Node Activation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Recall that the output of each node is simply a non-linear tranformation of the weighted sum. We use activation functions for this mapping. Let's use the sigmoid function as the activation function here. So let's define a function that takes a weighted sum as input and returns the non-linear transformation of the input using the sigmoid function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "def node_activation(weighted_sum):\n",
    "    return 1.0 / (1.0 + np.exp(-1 * weighted_sum))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "#### Use the *node_activation* function to compute the output of the first node in the first hidden layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "### type your answer here\n",
    "node_weights = network['layer_1']['node_1']['weights']\n",
    "node_bias = network['layer_1']['node_1']['bias']\n",
    "node_output = node_activation(compute_weighted_sum(inputs, node_weights, node_bias))\n",
    "print('<YourName>+ The output of the first node in the hidden layer is {}'.format(np.around(node_output[0], decimals=4)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<a id=\"item5\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='item15'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "## Forward Propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "The final piece of building a neural network that can perform predictions is to put everything together. So let's create a function that applies the *compute_weighted_sum* and *node_activation* functions to each node in the network and propagates the data all the way to the output layer and outputs a prediction for each node in the output layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "The way we are going to accomplish this is through the following procedure:\n",
    "\n",
    "1. Start with the input layer as the input to the first hidden layer.\n",
    "2. Compute the weighted sum at the nodes of the current layer.\n",
    "3. Compute the output of the nodes of the current layer.\n",
    "4. Set the output of the current layer to be the input to the next layer.\n",
    "5. Move to the next layer in the network.\n",
    "5. Repeat steps 2 - 4 until we compute the output of the output layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "def forward_propagate(network, inputs):\n",
    "    \n",
    "    layer_inputs = list(inputs) # start with the input layer as the input to the first hidden layer\n",
    "    \n",
    "    for layer in network:\n",
    "        \n",
    "        layer_data = network[layer]\n",
    "        \n",
    "        layer_outputs = [] \n",
    "        for layer_node in layer_data:\n",
    "        \n",
    "            node_data = layer_data[layer_node]\n",
    "        \n",
    "            # compute the weighted sum and the output of each node at the same time \n",
    "            node_output = node_activation(compute_weighted_sum(layer_inputs, node_data['weights'], node_data['bias']))\n",
    "            layer_outputs.append(np.around(node_output[0], decimals=4))\n",
    "            \n",
    "        if layer != 'output':\n",
    "            print('The outputs of the nodes in hidden layer number {} is {}'.format(layer.split('_')[1], layer_outputs))\n",
    "    \n",
    "        layer_inputs = layer_outputs # set the output of this layer to be the input to next layer\n",
    "\n",
    "    network_predictions = layer_outputs\n",
    "    return network_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "#### Use the *forward_propagate* function to compute the prediction of our small network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "### type your answser here\n",
    "# print(inputs)\n",
    "predictions = forward_propagate(network, inputs)\n",
    "print('The predicted value by the network for the given input is {}'.format(np.around(predictions, decimals=4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In order to store the activations of each layer, we can convert our forward_propagation as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "# Define the forward propagation\n",
    "def forward_propagationS(network, inputs):\n",
    "    \"\"\"\n",
    "    Perform forward propagation through the network.\n",
    "    Arguments:\n",
    "        inputs: Input array.\n",
    "        network: Network dictionary with weights and biases.\n",
    "    Returns:\n",
    "        activations: List of activations for all layers.\n",
    "    \"\"\"\n",
    "    activations = [inputs]  # Start with the input layer\n",
    "    for layer in network.values():\n",
    "        z = []\n",
    "        a = []\n",
    "        for node in layer.values():\n",
    "            z_value = np.dot(activations[-1], node['weights']) + node['bias']\n",
    "            z.append(z_value)\n",
    "            a.append(sigmoid(z_value))\n",
    "        activations.append(np.array(a).flatten())\n",
    "    return activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_s = forward_propagationS(network, inputs)\n",
    "predictions_s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "So we built the code to define a neural network. We can specify the number of inputs that a neural network can take, the number of hidden layers as well as the number of nodes in each hidden layer, and the number of nodes in the output layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Please use the *initialize_network* to create your neural network and define its weights and biases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# my_network = initialize_network(5, 3, [2, 3, 2], 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Then, for a given input,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# inputs = np.around(np.random.uniform(size=5), decimals=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "we compute the network predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# predictions = forward_propagate(my_network, inputs)\n",
    "# print('The predicted values by the network for the given input are {}'.format(predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Feel free to play around with the code by creating different networks of different structures and enjoy making predictions using the *forward_propagate* function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backward Propagation - Slides Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Backward propagation (backpropagation) is the process of computing gradients to adjust the weights and biases of a neural network, using the chain rule of differentiation. It is essential for training neural networks via optimization algorithms like gradient descent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the following example to derive and build the Backward Propagation function:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Slide example](https://raw.githubusercontent.com/Shangyue-CWU/CS457Draft/refs/heads/main/Neural_Example1.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialize Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the initial network parameters\n",
    "inputs = np.array([0.1, 0.5])  # Input values i1 and i2\n",
    "\n",
    "w1, w2, w3, w4 = 0.1, 0.2, 0.3, 0.4  # Weights from input to hidden\n",
    "w5, w6, w7, w8 = 0.5, 0.6, 0.7, 0.8  # Weights from hidden to output\n",
    "\n",
    "b1  = 0.25  # Biases for the hidden layer\n",
    "b2 = 0.35 # Biases for the output layer\n",
    "outputs = np.array([0.05, 0.95])  # True output values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Forward Propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward Propagation\n",
    "# Hidden layer\n",
    "z_h1 = w1 * inputs[0] + w3 * inputs[1] + b1\n",
    "z_h2 = w2 * inputs[0] + w4 * inputs[1] + b1\n",
    "h1 = sigmoid(z_h1)\n",
    "h2 = sigmoid(z_h2)\n",
    "\n",
    "# Output layer\n",
    "z_o1 = w5 * h1 + w7 * h2 + b2\n",
    "z_o2 = w6 * h1 + w8 * h2 + b2\n",
    "o1 = sigmoid(z_o1)\n",
    "o2 = sigmoid(z_o2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Backward Propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backward Propagation\n",
    "# Define sigmoid derivative function\n",
    "def sigmoid_derivative(a):\n",
    "    return a * (1 - a)\n",
    "\n",
    "# Compute errors at the output\n",
    "error_o1 = o1 - outputs[0]\n",
    "error_o2 = o2 - outputs[1]\n",
    "\n",
    "# Gradients for output layer weights\n",
    "d_w5 = error_o1 * sigmoid_derivative(o1) * h1\n",
    "d_w6 = error_o2 * sigmoid_derivative(o2) * h1\n",
    "d_w7 = error_o1 * sigmoid_derivative(o1) * h2\n",
    "d_w8 = error_o2 * sigmoid_derivative(o2) * h2\n",
    "\n",
    "\n",
    "print(\"Check parameters:\", error_o1 , sigmoid_derivative(o1) , h1,d_w5)\n",
    "\n",
    "learning_rate = 0.6\n",
    "# Update w5, w6, w7, 8\n",
    "w5 = w5 - learning_rate * d_w5\n",
    "w6 = w6 - learning_rate * d_w6\n",
    "w7 = w7 - learning_rate * d_w7\n",
    "w8 = w8 - learning_rate * d_w8\n",
    "\n",
    "\n",
    "print(\"Updated w5:\", w5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w6,w7,w8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backward Propagation for w1, w2, w3, w4\n",
    "# Deltas for output layer\n",
    "delta_o1 = (o1 - outputs[0]) * sigmoid_derivative(o1)\n",
    "delta_o2 = (o2 - outputs[1]) * sigmoid_derivative(o2)\n",
    "\n",
    "# Deltas for hidden layer\n",
    "delta_h1 = (delta_o1 * w5 + delta_o2 * w6) * sigmoid_derivative(h1)\n",
    "delta_h2 = (delta_o1 * w7 + delta_o2 * w8) * sigmoid_derivative(h2)\n",
    "\n",
    "# Gradients for input-to-hidden weights\n",
    "d_w1 = delta_h1 * inputs[0]\n",
    "d_w2 = delta_h2 * inputs[0]\n",
    "d_w3 = delta_h1 * inputs[1]\n",
    "d_w4 = delta_h2 * inputs[1]\n",
    "\n",
    "# Update weights\n",
    "w1 = w1 - learning_rate * d_w1\n",
    "w2 = w2 - learning_rate * d_w2\n",
    "w3 = w3 - learning_rate * d_w3\n",
    "w4 = w4 - learning_rate * d_w4\n",
    "\n",
    "# Print updated weights\n",
    "print(\"Updated w1:\", w1)\n",
    "print(\"Updated w2:\", w2)\n",
    "print(\"Updated w3:\", w3)\n",
    "print(\"Updated w4:\", w4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert to a function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We convert the above idea into a standard back-propagation function and feed back the weight of each level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backpropagation_with_updates(network, activations, y_true, learning_rate):\n",
    "    \"\"\"\n",
    "    Perform backpropagation to calculate gradients and update weights for each layer.\n",
    "    Arguments:\n",
    "        network: Dictionary containing layers with weights and biases.\n",
    "        activations: List of activations from forward propagation.\n",
    "        y_true: Ground truth labels.\n",
    "        learning_rate: Learning rate for weight updates.\n",
    "    Returns:\n",
    "        network: Updated network with modified weights and biases.\n",
    "    \"\"\"\n",
    "    # Step 1: Compute deltas for the output layer\n",
    "    # Calculate the error (delta) for the output layer by comparing the predicted output (activations[-1])\n",
    "    # with the true labels (y_true) and applying the derivative of the activation function.\n",
    "    delta_output = (activations[-1] - y_true) * sigmoid_derivative(activations[-1])\n",
    "\n",
    "    # Step 2: Backpropagate through each layer\n",
    "    # Iterate through the layers in reverse order (from output to input)\n",
    "    for layer_idx in reversed(range(len(network))):\n",
    "        # Get the current layer's name and data\n",
    "        layer_name = list(network.keys())[layer_idx]\n",
    "        current_layer = network[layer_name]\n",
    "\n",
    "        if layer_name == 'output':\n",
    "            # For the output layer, use the precomputed delta\n",
    "            delta = delta_output\n",
    "        else:\n",
    "            # For hidden layers, compute the delta by propagating the error backward\n",
    "            # Multiply the current delta with the weights of the next layer (transposed),\n",
    "            # then apply the derivative of the activation function.\n",
    "            \n",
    "            next_layer_name = list(network.keys())[layer_idx + 1]\n",
    "            next_layer = network[next_layer_name]\n",
    "            weight_matrix = np.array([node['weights'] for node in next_layer.values()])\n",
    "#             print('weight_matrix:',weight_matrix)\n",
    "            \n",
    "            #Apply the derivative of the activation function to scale the delta for the current layer.\n",
    "            # This ensures the gradient respects the activation function's behavior.\n",
    "            delta = np.dot(delta, weight_matrix.T) * sigmoid_derivative(activations[layer_idx + 1])\n",
    "\n",
    "        # Loop through each node in the current layer\n",
    "        for node_idx, (node_name, node_data) in enumerate(current_layer.items()):\n",
    "            # Get the activations from the previous layer (or input for the first hidden layer)\n",
    "            a_prev = activations[layer_idx]\n",
    "            \n",
    "            # Compute the gradients for weights (dW) and biases (db)\n",
    "            dW = delta[node_idx] * a_prev  # Gradient of weights\n",
    "            db = delta[node_idx]           # Gradient of biases\n",
    "\n",
    "            # Update weights and biases using gradient descent\n",
    "            current_layer[node_name]['weights'] -= learning_rate * dW  # Update weights\n",
    "            current_layer[node_name]['bias'] -= learning_rate * db     # Update bias\n",
    "\n",
    "    # Return the updated network with modified weights and biases\n",
    "    return network\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test our backpropagation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example inputs\n",
    "inputs = np.array([0.1, 0.5])  # Input layer\n",
    "y_true = np.array([0.05, 0.95])  # Target output\n",
    "learning_rate = 0.6  # Learning rate\n",
    "\n",
    "# Manually initialize network\n",
    "network = {\n",
    "    'layer_1': {\n",
    "        'node_1': {'weights': np.array([0.1, 0.3]), 'bias': np.array([0.25])},\n",
    "        'node_2': {'weights': np.array([0.2, 0.4]), 'bias': np.array([0.25])},\n",
    "    },\n",
    "    'output': {\n",
    "        'node_1': {'weights': np.array([0.5, 0.7]), 'bias': np.array([0.35])},\n",
    "        'node_2': {'weights': np.array([0.6, 0.8]), 'bias': np.array([0.35])},\n",
    "    },\n",
    "}\n",
    "\n",
    "# Perform forward propagation\n",
    "activations = forward_propagationS(network,inputs)\n",
    "print(\"activations with the first forward pass: \",activations)\n",
    "\n",
    "# Perform backpropagation and weight updates\n",
    "updated_network = backpropagation_with_updates(network, activations, y_true, learning_rate)\n",
    "\n",
    "# Print updated weights and biases\n",
    "print(\"<YourName>+Updated Weights and Biases:\")\n",
    "for layer_name, layer_nodes in updated_network.items():\n",
    "    print(f\"Layer: {layer_name}\")\n",
    "    for node_name, node_data in layer_nodes.items():\n",
    "        print(f\"  Node: {node_name}, Weights: {node_data['weights']}, Bias: {node_data['bias']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize the change of loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Compute the Mean Squared Error (MSE) loss.\n",
    "    Arguments:\n",
    "        y_true: Ground truth labels (numpy array).\n",
    "        y_pred: Predicted output from the network (numpy array).\n",
    "    Returns:\n",
    "        Mean Squared Error (scalar value).\n",
    "    \"\"\"\n",
    "    return np.mean((y_true - y_pred) ** 2)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Training Loop\n",
    "losses = []  # List to store the loss at each iteration\n",
    "learning_rate = 0.6  # Learning rate for weight updates\n",
    "\n",
    "# Iterate over the number of training steps (100 iterations in this case)\n",
    "for iteration in range(100):\n",
    "    # Step 1: Perform forward propagation to compute the network's predictions\n",
    "    activations = forward_propagationS(network, inputs)  # Forward pass\n",
    "    \n",
    "    # Step 2: Compute the loss (error) between the true labels and the predicted output\n",
    "    loss = compute_loss(y_true, activations[-1])  # Loss for the current iteration\n",
    "    losses.append(loss)  # Append the computed loss to the list\n",
    "    \n",
    "    # Step 3: Perform backpropagation to compute gradients and update weights\n",
    "    network = backpropagation_with_updates(network, activations, y_true, learning_rate)\n",
    "\n",
    "# Plot Loss\n",
    "plt.plot(losses)  \n",
    "plt.title(\"<YourName>+Loss Before and After Weight Updates\") \n",
    "plt.xlabel(\"Iterations\")  \n",
    "plt.ylabel(\"Loss\")  \n",
    "plt.show()  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
